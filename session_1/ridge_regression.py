import numpy as np


def load_data(data_path):
    """
    Äá»c dá»¯ liá»‡u tá»« file death_rates-data.txt
    :param data_path: Ä‘Æ°á»ng dáº«n Ä‘áº¿n file
    :return:
        X: máº£ng 2 chiá»u biá»ƒu diá»…n táº­p thuá»™c tÃ­nh
        má»—i hÃ ng tÆ°Æ¡ng á»©ng vá»›i 1 Ä‘iá»ƒm dá»¯ liá»‡u
        Y: máº£ng 1 chiá»u biá»ƒu diá»…n cÃ¡c giÃ¡ trá»‹
        death rate tÆ°Æ¡ng á»©ng vá»›i tá»«ng Ä‘iá»ƒm dá»¯ liá»‡u
    """
    with open(data_path) as file:
        data = np.loadtxt(file)
        X = data[:, 1:16]
        Y = data[:, 16:]
    return X, Y


def normalize_and_add_ones(X):
    """
    Chuáº©n hÃ³a dá»¯ liá»‡u sá»­ dá»¥ng feature scaling
    :param X: Táº­p thuá»™c tÃ­nh Ä‘áº§u vÃ o
    :return: táº­p thuá»™c tÃ­nh Ä‘Ã£ chuáº©n hÃ³a vÃ  thÃªm vector cá»™t [1 1 ... 1] vÃ o
        Ä‘áº§u (Ä‘á»ƒ nhÃ¢n vá»›i bias w0)
    """
    X_max = np.array([[np.amax(X[:, column_id])
                       for column_id in range(X.shape[1])]
                      for _ in range(X.shape[0])])
    X_min = np.array([[np.amin(X[:, column_id])
                       for column_id in range(X.shape[1])]
                      for _ in range(X.shape[0])])

    X_normalized = (X - X_min) / (X_max - X_min)

    ones = np.array([[1] for _ in range(X.shape[0])])
    return np.column_stack((ones, X_normalized))


class RidgeRegression:
    def __init__(self):
        return

    def fit(self, X_train, Y_train, LAMBDA):
        """
        TÃ¬m nghiá»‡m báº±ng cÃ´ng thá»©c nghiá»‡m ğ‘¤âˆ—=(ğ‘‹^ğ‘‡ * ğ‘‹ + ğœ† * ğ¼_(ğ¾+1))^(âˆ’1) * ğ‘‹ğ‘Œ
        :param X_train: táº­p thuá»™c tÃ­nh
        :param Y_train: táº­p giÃ¡ trá»‹ Ä‘áº§u ra tÆ°Æ¡ng á»©ng vá»›i tá»«ng vector thuá»™c tÃ­nh
        :param LAMBDA: weight decay, siÃªu tham sá»‘, 0 < LAMBDA < 1
        :return: vector tham sá»‘ W
        """
        assert len(X_train.shape) == 2 and X_train.shape[0] == Y_train.shape[0]
        W = np.linalg.inv(np.transpose(X_train).dot(X_train) + \
                          LAMBDA * np.identity(X_train.shape[1])
                          ).dot(np.transpose(X_train)).dot(Y_train)
        return W

    def fit_gradient_descent(self, X_train, Y_train, LAMBDA, lr, max_num_epochs=100, batch_size=128):
        """
        TÃ¬m nghiá»‡m báº±ng thuáº­t toÃ¡n gradient
        :param X_train: táº­p thuá»™c tÃ­nh
        :param Y_train: táº­p giÃ¡ trá»‹ Ä‘áº§u ra tÆ°Æ¡ng á»©ng vá»›i tá»«ng vector thuá»™c tÃ­nh
        :param LAMBDA: weight decay, siÃªu tham sá»‘
        :param lr: learning rate, siÃªu tham sá»‘
        :param max_num_epochs: sá»‘ lÆ°á»£ng epoch khi train
        :param batch_size: kÃ­ch thÆ°á»›c batch
        :return: vector tham sá»‘ W
        """
        W = np.random.rand(X_train.shape[1])
        W = W.reshape((X_train.shape[1], 1))
        lass_lost = 10e+8
        for epoch in range(max_num_epochs):
            # xÃ¡o trá»™n táº­p dá»¯ liá»‡u khi báº¯t Ä‘áº§u 1 epoch má»›i
            arr = np.array(range(X_train.shape[0]))
            np.random.shuffle(arr)
            X_train = X_train[arr]
            Y_train = Y_train[arr]
            # sá»‘ batch trÃªn 1 epoch
            total_minibatch = int(np.ceil(X_train.shape[0] / batch_size))
            for i in range(total_minibatch):
                index = i * batch_size
                X_train_sub = X_train[index:index + batch_size]
                Y_train_sub = Y_train[index:index + batch_size]
                # tÃ­nh gradient cá»§a hÃ m loss
                grad = np.transpose(X_train_sub).dot(X_train_sub.dot(W) - Y_train_sub) + LAMBDA * W
                # cáº­p nháº­t trá»ng sá»‘ W
                W = W - lr * grad
            # Ä‘iá»u kiá»‡n dá»«ng: sau má»—i epoch náº¿u giÃ¡ trá»‹ thay Ä‘á»•i cá»§a hÃ m loss
            # khÃ´ng quÃ¡ 10^-5 thÃ¬ káº¿t thÃºc quÃ¡ trÃ¬nh train
            new_loss = self.compute_RSS(self.predict(W, X_train), Y_train)
            if np.abs(new_loss - lass_lost) <= 1e-5:
                break
            lass_lost = new_loss
        return W

    def predict(self, W, X_new):
        """
        ÄÆ°a ra dá»± Ä‘oÃ¡n gÃ¡i trá»‹ death rate Y á»©ng vá»›i trá»ng sá»‘ W vÃ  Ä‘áº§u vÃ o X
        :param W: trá»ng sá»‘ cá»§a mÃ´ hÃ¬nh
        :param X_new: giÃ¡ trá»‹ input
        :return: giÃ¡ trá»‹ dá»± Ä‘oÃ¡n cá»§a Y
        """
        return np.array(X_new).dot(W)

    def compute_RSS(self, Y_new, Y_predicted):
        """
        TÃ­nh giÃ¡ trá»‹ hÃ m loss
        :param Y_new: giÃ¡ trá»‹ Y thá»±c táº¿
        :param Y_predicted: GiÃ¡ trá»‹ Y dá»± Ä‘oÃ¡n bá»Ÿi mÃ´ hÃ¬nh
        :return: GiÃ¡ trá»‹ hÃ m MSE loss
        """
        return 1.0 / Y_new.shape[0] * np.sum((Y_new - Y_predicted) ** 2)

    def get_the_best_LAMBDA(self, X_train, Y_train):
        """
        Lá»±a chá»n tham sá»‘ Lambda báº±ng phÆ°Æ¡ng phÃ¡p k-fold cross validation
        :param X_train, Y_train: Táº­p cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  nhÃ£n
        :return: giÃ¡ trá»‹ Lambda lÃ m cho hÃ m loss Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t trÃªn
            táº­p huáº¥n luyá»‡n
        """

        def cross_validation(num_folds, LAMBDA):
            # chia táº­p dá»¯ liá»‡u huáº¥n luyá»‡n thÃ nh k pháº§n
            row_ids = np.array(range(X_train.shape[0]))
            valid_ids = np.split(row_ids[:(len(row_ids) - len(row_ids) % num_folds)], num_folds)
            valid_ids[-1] = np.append(valid_ids[-1], row_ids[len(row_ids) - len(row_ids) % num_folds:])
            train_ids = [[k for k in row_ids if k not in valid_ids[i]] for i in range(num_folds)]
            aver_RSS = 0
            for i in range(num_folds):
                # vá»›i má»—i fold láº¥y 1 pháº§n lÃ m táº­p val. k - 1 pháº§n lÃ m táº­p train
                valid_part = {'X': X_train[valid_ids[i]], 'Y': Y_train[valid_ids[i]]}
                train_part = {'X': X_train[train_ids[i]], 'Y': Y_train[train_ids[i]]}
                # Ä‘Æ°a ra giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  tÃ­nh hÃ m loss`
                W = self.fit(train_part['X'], train_part['Y'], LAMBDA)
                Y_predict = self.predict(W, valid_part['X'])
                aver_RSS += self.compute_RSS(valid_part['Y'], Y_predict)
            return aver_RSS / num_folds

        # chá»n Lambda cÃ³ giÃ¡ trá»‹ loss tháº¥p nháº¥t
        def range_scan(best_LAMBDA, minimum_RSS, LAMBDA_values):
            for current_LAMBDA in LAMBDA_values:
                aver_RSS = cross_validation(num_folds=5, LAMBDA=current_LAMBDA)
                if aver_RSS < minimum_RSS:
                    best_LAMBDA = current_LAMBDA
                    minimum_RSS = aver_RSS
            return best_LAMBDA, minimum_RSS

        # cháº¡y láº§n 1 Ä‘á»ƒ chá»n cáº­n trÃªm cá»§a hÃ m loss
        best_LAMBDA, minimum_RSS = range_scan(best_LAMBDA=0,
                                              minimum_RSS=10000 ** 2,
                                              LAMBDA_values=[i * 1.0 / 1000 for i in range(1, 1000, 1)])
        # print('Best LAMBDA: ', best_LAMBDA)
        # print('Minimum RSS: ', minimum_RSS)
        # cháº¡y láº§n 2 Ä‘á»ƒ Ä‘Æ°a ra giÃ¡ trá»‹ Lambda tá»‘t nháº¥t
        best_LAMBDA, minimum_RSS = range_scan(best_LAMBDA=best_LAMBDA,
                                              minimum_RSS=minimum_RSS,
                                              LAMBDA_values=[i * 5.0 / 10000 for i in range(1, 2000, 1)])

        return best_LAMBDA

    def get_the_best_lr(self, X_train, Y_train, LAMBDA):
        """
        Lá»±a chá»n learning rate báº±ng phÆ°Æ¡ng phÃ¡p k-fold cross validation
        :param X_train, Y_train: Táº­p cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u huáº¥n luyá»‡n vÃ  nhÃ£n
        :param LAMBDA:
        :return: giÃ¡ trá»‹ Lambda lÃ m cho hÃ m loss Ä‘áº¡t giÃ¡ trá»‹ nhá» nháº¥t trÃªn
            táº­p huáº¥n luyá»‡n
        """

        def cross_validation(num_folds, lr):
            # chia táº­p dá»¯ liá»‡u huáº¥n luyá»‡n thÃ nh k pháº§n
            row_ids = np.array(range(X_train.shape[0]))
            valid_ids = np.split(row_ids[:(len(row_ids) - len(row_ids) % num_folds)], num_folds)
            valid_ids[-1] = np.append(valid_ids[-1], row_ids[len(row_ids) - len(row_ids) % num_folds:])
            train_ids = [[k for k in row_ids if k not in valid_ids[i]] for i in range(num_folds)]
            aver_RSS = 0
            for i in range(num_folds):
                # vá»›i má»—i fold láº¥y 1 pháº§n lÃ m táº­p val. k - 1 pháº§n lÃ m táº­p train
                valid_part = {'X': X_train[valid_ids[i]], 'Y': Y_train[valid_ids[i]]}
                train_part = {'X': X_train[train_ids[i]], 'Y': Y_train[train_ids[i]]}
                # Ä‘Æ°a ra giÃ¡ trá»‹ dá»± Ä‘oÃ¡n vÃ  tÃ­nh hÃ m loss`
                W = self.fit_gradient_descent(train_part['X'], train_part['Y'], LAMBDA, lr)
                Y_predict = self.predict(W, valid_part['X'])
                aver_RSS += self.compute_RSS(valid_part['Y'], Y_predict)
            return aver_RSS / num_folds

        # chá»n Lambda cÃ³ giÃ¡ trá»‹ loss tháº¥p nháº¥t
        def range_scan(best_lr, minimum_RSS, lr_values):
            for current_lr in lr_values:
                aver_RSS = cross_validation(num_folds=5, lr=current_lr)
                if aver_RSS < minimum_RSS:
                    best_lr = current_lr
                    minimum_RSS = aver_RSS
            return best_lr, minimum_RSS

        # cháº¡y láº§n 1 Ä‘á»ƒ chá»n cáº­n trÃªm cá»§a hÃ m loss
        best_lr, minimum_RSS = range_scan(best_lr=0,
                                          minimum_RSS=10000 ** 2,
                                          lr_values=[i * 1.0 / 1000 for i in range(1, 1000, 1)])
        # print('Best lr: ', best_lr)
        # print('Minimum RSS: ', minimum_RSS)
        # cháº¡y láº§n 2 Ä‘á»ƒ Ä‘Æ°a ra giÃ¡ trá»‹ lr tá»‘t nháº¥t
        # best_lr, minimum_RSS = range_scan(best_lr=best_lr,
        #                                   minimum_RSS=minimum_RSS,
        #                                   lr_values=[i * 5.0 / 10000 for i in range(1, 2000, 1)])

        return best_lr


if __name__ == '__main__':
    X, Y = load_data(data_path='datasets/death-rates-data.txt')
    X = normalize_and_add_ones(X)
    # chia táº­p dá»¯ liá»‡u thÃ nh train vÃ  test
    X_train, Y_train = X[:50], Y[:50]
    X_test, Y_test = X[50:], Y[50:]

    ridge_regression = RidgeRegression()
    LAMBDA = ridge_regression.get_the_best_LAMBDA(X_train, Y_train)
    print('Best LAMBDA: ', LAMBDA)
    # sá»­ dá»¥ng cÃ´ng thá»©c nghiá»‡m
    W_learned = ridge_regression.fit(X_train, Y_train, LAMBDA)
    Y_predicted = ridge_regression.predict(W=W_learned, X_new=X_test)
    print('Loss khi dÃ¹ng cÃ´ng thá»©c nghiá»‡m', ridge_regression.compute_RSS(Y_new=Y_test, Y_predicted=Y_predicted))

    # sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p gradient
    lr = ridge_regression.get_the_best_lr(X_train, Y_train, LAMBDA)
    print('Best learning rate: ', lr)
    W_gradient = ridge_regression.fit_gradient_descent(X_train, Y_train, LAMBDA, lr, batch_size=4)
    Y_predicted_gradient = ridge_regression.predict(W=W_gradient, X_new=X_test)

    print('Loss khi dÃ¹ng gradient', ridge_regression.compute_RSS(Y_new=Y_test, Y_predicted=Y_predicted_gradient))
